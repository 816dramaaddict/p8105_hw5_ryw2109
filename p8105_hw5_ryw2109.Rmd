---
title: "p8105_hw5_ryw2109"
author: "Rita Wang"
date: "2024-11-15"
output: github_document
---

```{r default_setting, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
library(tidyverse)
library(ggplot2)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r q1_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1) #  fixes the output.

birthday_check = function(dob){
  birthdays <- sample(1:365, dob, replace = TRUE)
    # fixed group size
    # randomly draws â€œbirthdaysâ€ for each person
  return(any(duplicated(birthdays)))
    # checks whether there are duplicate birthdays in the group
    # returns TRUE or FALSE based on the result
}

# Running this function 10000 times for each group size between 2 and 50
  stimulation = 10000 
  group_size = 2:50

# For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs
  probability = sapply(group_size, function(n) {
    mean(replicate(stimulation, birthday_check(n)))
  })

dob_data = data.frame(group_size,probability)
  # preparing for graphing
  
# Make a plot showing the probability as a function of group size, and comment on your results.
ggplot(dob_data, aes(x = group_size, y = probability)) +
  geom_line(color = "purple") +
  geom_point(color = "#90EE90") +
  labs(
    title = "Probability of Shared Birthday VS Group Size",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  )
```

Given a function that computes the probability that at least two people share a birthday for a fixed group size between 2 and 50, after running 10,000 times, the probability of shared birthdays as the group size increase is shown above.

```{r q2_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1)

n = 30 # Fix ð‘›=30
sigma = 5 # Fix ðœŽ=5
alpha = 0.05
mu_values = 0:6
simulation = 5000 # Generate 5000 datasets from the model

# For each dataset, save ðœ‡Ì‚ and the p-value arising from a test of ð»:ðœ‡=0 using ð›¼=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.
perform_ttest = function(mu, n, sigma, simulation, alpha) {
  results = replicate(simulation, {
  x = rnorm(n, mean = mu, sd = sigma) # Generate dataset
  test_result = t.test(x, mu = 0) # t-test
  tidy_result = broom::tidy(test_result) # cleaning output of t.test
  p_value = tidy_result$p.value
  estimate = tidy_result$estimate # estimate of the mean
  return(c(p_value = p_value, estimate = estimate))
})

results_df = data.frame(t(results)) # results to df
colnames(results_df) = c("p_value", "estimate")
  
rejection_rate = mean(results_df$p_value < alpha) # power
  
avg_estimate = mean(results_df$estimate) # average estimate
  
return(list(rejection_rate = rejection_rate, avg_estimate = avg_estimate, results = results_df))
}

power_results = lapply(mu_values, function(mu) { # simulation
  perform_ttest(mu, n, sigma, simulation, alpha)
})

rejection_rates = sapply(power_results, function(res) res$rejection_rate) # extracting rejection rates
avg_estimates = sapply(power_results, function(res) res$avg_estimate) # extracting average estimates

```

```{r q2_b, echo = FALSE, message = FALSE, warning = FALSE}
power_df = data.frame(mu = mu_values, power = rejection_rates)

ggplot(power_df, aes(x = mu, y = power)) +
  geom_line(color = "#ade8f8")
  geom_point()
  labs(
    title = "Power of the Test vs. True Mean (mu)",
    x = "True Mean (mu)",
    y = "Power (Proportion Rejected)")
```

The power increases for effect sizes 0 to 4, however, for effect sizes 4 to 6, the power stayed relatively similar. The increase of power was greatest for effect size 1 to 3. With the increase in effect size, there is an increase of power, which in turn indicates higher probability of rejecting the null hypothesis.

```{r q2_c, echo = FALSE, message = FALSE, warning = FALSE}
plot_data = data.frame(
  mu = mu_values,
  avg_estimate = avg_estimates,
  rejection_rates = rejection_rates
)

ggplot(plot_data, aes(x = mu)) +
      # Plot the average estimate
  geom_line(aes(y = avg_estimate, color = "Average Estimate")) +
  geom_point(aes(y = avg_estimate, color = "Average Estimate")) +
      # Plot the average estimate when the null was rejected
  
  geom_line(aes(y = rejection_rates, color = "Rejection Rate"), linetype = "dashed") +
  geom_point(aes(y = rejection_rates, color = "Rejection Rate")) +
  labs(
    title = "Average Estimate of ÂµÌ‚ vs. True Âµ with Rejection Condition",
    x = "True Mean (Âµ)",
    y = "Estimated Mean (ÂµÌ‚)",
    color = "Legend")
```

The sample average ðœ‡ across tests for which the null is rejected is not approximately equal to the true value of ðœ‡. This may be due to sample variability and Type I error.
```{r q3_a, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
  # importing data
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"

directory = "./data"

if (!dir.exists(directory)) {
  dir.create(directory)
}

file_path = file.path(directory, "homicide-data.csv")
download.file(url, destfile = file_path, method = "libcurl")

data_original = read.csv(file_path) %>% 
  janitor::clean_names()
```

The data provided has a total of `r nrow(data_original)` rows and `r ncol(data_original)` columns. The dataset consists of the following data information:

`r names(data_original)`

```{r q3_b, echo = FALSE, message = FALSE, warning = FALSE}
data_df1 = data_original %>% 
  mutate(
    city_state = paste(city, state, sep = ", ")
  )

data_df2 = data_df1 %>%
  group_by(city_state) %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )
```

```{r q3_c, echo = FALSE, message = FALSE, warning = FALSE}
md = data_df2 %>% 
  filter(city_state == "Baltimore, MD")

prop_test = prop.test(md$unsolved_homicides, md$total_homicides)

md_prop = broom::tidy(prop_test) %>% # cleaning
  select(estimate, conf.low, conf.high) # Extracting the proportion and confidence intervals

city_props = data_df2 %>% # prop.test for each city
  mutate(
    city_prop_test = map2(unsolved_homicides,
                             total_homicides,
                             ~broom::tidy(prop.test(.x, .y)))
    ) %>%
  unnest_wider(city_prop_test) %>%
  select(city_state, estimate, conf.low, conf.high)

city_props
```

```{r q3_d, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(city_props, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(color = "orange") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) #*
```
