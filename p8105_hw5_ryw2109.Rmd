---
title: "p8105_hw5_ryw2109"
author: "Rita Wang"
date: "2024-11-15"
output: github_document
---

```{r default_setting, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
library(tidyverse)
library(ggplot2)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r q1_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1) #  fixes the output.

birthday_check = function(dob){
  birthdays <- sample(1:365, dob, replace = TRUE)
    # fixed group size
    # randomly draws “birthdays” for each person
  return(any(duplicated(birthdays)))
    # checks whether there are duplicate birthdays in the group
    # returns TRUE or FALSE based on the result
}

# Running this function 10000 times for each group size between 2 and 50
  stimulation = 10000 
  group_size = 2:50

# For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs
  probability = sapply(group_size, function(n) {
    mean(replicate(stimulation, birthday_check(n)))
  })

dob_data = data.frame(group_size,probability)
  # preparing for graphing
  
# Make a plot showing the probability as a function of group size, and comment on your results.
ggplot(dob_data, aes(x = group_size, y = probability)) +
  geom_line(color = "purple") +
  geom_point(color = "#90EE90") +
  labs(
    title = "Probability of Shared Birthday VS Group Size",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  )
```

Given a function that computes the probability that at least two people share a birthday for a fixed group size between 2 and 50, after running 10,000 times, the probability of shared birthdays as the group size increase is shown above.

```{r q2_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1)

n = 30 # Fix 𝑛=30
sigma = 5 # Fix 𝜎=5
alpha = 0.05
mu_values = 0:6
simulation = 5000 # Generate 5000 datasets from the model

# For each dataset, save 𝜇̂ and the p-value arising from a test of 𝐻:𝜇=0 using 𝛼=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.
perform_ttest = function(mu, n, sigma, simulation, alpha) {
  results = replicate(simulation, {
  x = rnorm(n, mean = mu, sd = sigma) # Generate dataset
  test_result = t.test(x, mu = 0) # t-test
  tidy_result = broom::tidy(test_result) # cleaning output of t.test
  p_value = tidy_result$p.value
  estimate = tidy_result$estimate # estimate of the mean
  return(c(p_value = p_value, estimate = estimate))
})

results_df = data.frame(t(results)) # results to df
colnames(results_df) = c("p_value", "estimate")
  
rejection_rate = mean(results_df$p_value < alpha) # power
  
avg_estimate = mean(results_df$estimate) # average estimate
  
return(list(rejection_rate = rejection_rate, avg_estimate = avg_estimate, results = results_df))
}

power_results = lapply(mu_values, function(mu) { # simulation
  perform_ttest(mu, n, sigma, simulation, alpha)
})

rejection_rates = sapply(power_results, function(res) res$rejection_rate) # extracting rejection rates
avg_estimates = sapply(power_results, function(res) res$avg_estimate) # extracting average estimates

```

```{r q2_b, echo = FALSE, message = FALSE, warning = FALSE}
power_df = data.frame(mu = mu_values, power = rejection_rates)

ggplot(power_df, aes(x = mu, y = power)) +
  geom_line(color = "#ade8f8")
  geom_point()
  labs(
    title = "Power of the Test vs. True Mean (mu)",
    x = "True Mean (mu)",
    y = "Power (Proportion Rejected)")
```

The power increases for effect sizes 0 to 4, however, for effect sizes 4 to 6, the power stayed relatively similar. The increase of power was greatest for effect size 1 to 3. With the increase in effect size, there is an increase of power, which in turn indicates higher probability of rejecting the null hypothesis.

```{r q2_c, echo = FALSE, message = FALSE, warning = FALSE}
plot_data = data.frame(
  mu = mu_values,
  avg_estimate = avg_estimates,
  rejection_rates = rejection_rates
)

ggplot(plot_data, aes(x = mu)) +
      # Plot the average estimate
  geom_line(aes(y = avg_estimate, color = "Average Estimate")) +
  geom_point(aes(y = avg_estimate, color = "Average Estimate")) +
      # Plot the average estimate when the null was rejected
  
  geom_line(aes(y = rejection_rates, color = "Rejection Rate"), linetype = "dashed") +
  geom_point(aes(y = rejection_rates, color = "Rejection Rate")) +
  labs(
    title = "Average Estimate of µ̂ vs. True µ with Rejection Condition",
    x = "True Mean (µ)",
    y = "Estimated Mean (µ̂)",
    color = "Legend")
```

The sample average 𝜇 across tests for which the null is rejected is not approximately equal to the true value of 𝜇. This may be due to sample variability and Type I error.
```{r q3_a, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
  # importing data
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"

directory = "./data"

if (!dir.exists(directory)) {
  dir.create(directory)
}

file_path = file.path(directory, "homicide-data.csv")
download.file(url, destfile = file_path, method = "libcurl")

data_original = read.csv(file_path) %>% 
  janitor::clean_names()
```

The data provided has a total of `r nrow(data_original)` rows and `r ncol(data_original)` columns. The dataset consists of the following data information:

`r names(data_original)`

```{r q3_b, echo = FALSE, message = FALSE, warning = FALSE}
data_df1 = data_original %>% 
  mutate(
    city_state = paste(city, state, sep = ", ")
  )

data_df2 = data_df1 %>%
  group_by(city_state) %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )
```

```{r q3_c, echo = FALSE, message = FALSE, warning = FALSE}
md = data_df2 %>% 
  filter(city_state == "Baltimore, MD")

prop_test = prop.test(md$unsolved_homicides, md$total_homicides)

md_prop = broom::tidy(prop_test) %>% # cleaning
  select(estimate, conf.low, conf.high) # Extracting the proportion and confidence intervals

city_props = data_df2 %>% # prop.test for each city
  mutate(
    city_prop_test = map2(unsolved_homicides,
                             total_homicides,
                             ~broom::tidy(prop.test(.x, .y)))
    ) %>%
  unnest_wider(city_prop_test) %>%
  select(city_state, estimate, conf.low, conf.high)

city_props
```

```{r q3_d, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(city_props, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(color = "orange") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) #*
```
