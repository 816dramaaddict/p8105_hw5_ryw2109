---
title: "p8105_hw5_ryw2109"
author: "Rita Wang"
date: "2024-11-15"
output: github_document
---

```{r default_setting, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
library(tidyverse)
library(ggplot2)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r q1_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1) #  fixes the output.

birthday_check = function(dob){
  birthdays <- sample(1:365, dob, replace = TRUE)
    # fixed group size
    # randomly draws “birthdays” for each person
  return(any(duplicated(birthdays)))
    # checks whether there are duplicate birthdays in the group
    # returns TRUE or FALSE based on the result
}

# Running this function 10000 times for each group size between 2 and 50
  stimulation = 10000 
  group_size = 2:50

# For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs
  probability = sapply(group_size, function(n) {
    mean(replicate(stimulation, birthday_check(n)))
  })

dob_data = data.frame(group_size,probability)
  # preparing for graphing
  
# Make a plot showing the probability as a function of group size, and comment on your results.
ggplot(dob_data, aes(x = group_size, y = probability)) +
  geom_line(color = "purple") +
  geom_point(color = "#90EE90") +
  labs(
    title = "Probability of Shared Birthday VS Group Size",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  )
```

Given a function that computes the probability that at least two people share a birthday for a fixed group size between 2 and 50, after running 10,000 times, the probability of shared birthdays as the group size increase is shown above.

```{r q2_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1)

n = 30 # Fix 𝑛=30
sigma = 5 # Fix 𝜎=5
alpha = 0.05
mu_values = 0:6
simulation = 5000 # Generate 5000 datasets from the model

# For each dataset, save 𝜇̂ and the p-value arising from a test of 𝐻:𝜇=0 using 𝛼=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.
perform_ttest = function(mu, n, sigma, simulation, alpha) {
  results = replicate(simulation, {
  x = rnorm(n, mean = mu, sd = sigma) # Generate dataset
  test_result = t.test(x, mu = 0) # t-test
  tidy_result = broom::tidy(test_result) # cleaning output of t.test
  p_value = tidy_result$p.value
  estimate = tidy_result$estimate # estimate of the mean
  return(c(p_value = p_value, estimate = estimate))
})

results_df = data.frame(t(results)) # results to df
colnames(results_df) = c("p_value", "estimate")
  
rejection_rate = mean(results_df$p_value < alpha) # power
  
avg_estimate = mean(results_df$estimate) # average estimate
  
return(list(rejection_rate = rejection_rate, avg_estimate = avg_estimate, results = results_df))
}

power_results = lapply(mu_values, function(mu) { # simulation
  perform_ttest(mu, n, sigma, simulation, alpha)
})

rejection_rates = sapply(power_results, function(res) res$rejection_rate) # extracting rejection rates
avg_estimates = sapply(power_results, function(res) res$avg_estimate) # extracting average estimates

```

```{r q2_b, echo = FALSE, message = FALSE, warning = FALSE}
power_df = data.frame(mu = mu_values, power = rejection_rates)

power_plot = ggplot(power_df, aes(x = mu, y = power)) +
  geom_line(color = "#ade8f8")
  geom_point()
  labs(title = "Power of the Test vs. True Mean (mu)",
       x = "True Mean (mu)",
       y = "Power (Proportion Rejected)")
  
power_plot
```

The power increases for effect sizes 0 to 4, however, for effect sizes 4 to 6, the power stayed relatively similar. The increase of power was greatest for effect size 1 to 3. With the increase in effect size, there is an increase of power, which in turn indicates higher probability of rejecting the null hypothesis.

```{r q2_c, echo = FALSE, message = FALSE, warning = FALSE}
plot_data = data.frame(
  mu = mu_values,
  avg_estimate = avg_estimates,
  rejection_rates = rejection_rates
)

plot = ggplot(plot_data, aes(x = mu)) +
      # Plot the average estimate
  geom_line(aes(y = avg_estimate, color = "Average Estimate")) +
  geom_point(aes(y = avg_estimate, color = "Average Estimate")) +
      # Plot the average estimate when the null was rejected
  
  geom_line(aes(y = rejection_rates, color = "Rejection Rate"), linetype = "dashed") +
  geom_point(aes(y = rejection_rates, color = "Rejection Rate")) +
  labs(title = "Average Estimate of µ̂ vs. True µ with Rejection Condition",
       x = "True Mean (µ)",
       y = "Estimated Mean (µ̂)",
       color = "Legend")

plot
```

The sample average 𝜇 across tests for which the null is rejected is not approximately equal to the true value of 𝜇. This may be due to sample variability and Type I error.
```{r q3_a, echo = FALSE, message = FALSE, warning = FALSE}
  # importing data
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"
data_original = read.csv(url) %>% 
  janitor::clean_names()

data_original

# Assuming the dataset has columns 'city', 'state', 'disposition', and 'homicide_id'
# If the dataset is already in your environment, move to step 2.

# 2. Create a new city_state variable (city + state)
homicide_data <- homicide_data %>%
  mutate(city_state = paste(city, state, sep = ", "))

# 3. Summarize within cities to get total homicides and unsolved homicides
city_summary <- homicide_data %>%
  group_by(city_state) %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

# 4. Estimate the proportion of unsolved homicides for Baltimore, MD
baltimore_data <- city_summary %>% filter(city_state == "Baltimore, MD")
prop_test_baltimore <- prop.test(baltimore_data$unsolved_homicides, baltimore_data$total_homicides)

# Tidy the prop.test output and extract the proportion and confidence intervals
baltimore_prop <- tidy(prop_test_baltimore) %>%
  select(estimate, conf.low, conf.high)

# 5. Run prop.test for each city and extract proportions and confidence intervals
city_props <- city_summary %>%
  pmap_dfr(function(city, total, unsolved) {
    prop_test <- prop.test(unsolved, total)
    tidy_result <- tidy(prop_test)
    
    data.frame(
      city_state = city,
      estimate = tidy_result$estimate,
      conf.low = tidy_result$conf.low,
      conf.high = tidy_result$conf.high
    )
  })

# 6. Create a plot showing estimates and CIs for each city
ggplot(city_props, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  coord_flip() +  # Flip coordinates to make the plot more readable
  theme_minimal()
```