---
title: "p8105_hw5_ryw2109"
author: "Rita Wang"
date: "2024-11-15"
output: github_document
---

```{r default_setting, echo = FALSE, message = FALSE, warning = FALSE, results = 'hide'}
library(tidyverse)
library(ggplot2)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r q1_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1) #  fixes the output.

birthday_check = function(dob){
  birthdays <- sample(1:365, dob, replace = TRUE)
    # fixed group size
    # randomly draws â€œbirthdaysâ€ for each person
  return(any(duplicated(birthdays)))
    # checks whether there are duplicate birthdays in the group
    # returns TRUE or FALSE based on the result
}

# Running this function 10000 times for each group size between 2 and 50
  stimulation = 10000 
  group_size = 2:50

# For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs
  probability = sapply(group_size, function(n) {
    mean(replicate(stimulation, birthday_check(n)))
  })

dob_data = data.frame(group_size,probability)
  # preparing for graphing
  
# Make a plot showing the probability as a function of group size, and comment on your results.
ggplot(dob_data, aes(x = group_size, y = probability)) +
  geom_line(color = "purple") +
  geom_point(color = "#90EE90") +
  labs(
    title = "Probability of Shared Birthday VS Group Size",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  )
```

Given a function that computes the probability that at least two people share a birthday for a fixed group size between 2 and 50, after running 10,000 times, the probability of shared birthdays as the group size increase is shown above.

```{r q2_a, echo = FALSE, message = FALSE, warning = FALSE}
set.seed(1)

n = 30 # Fix ð‘›=30
sigma = 5 # Fix ðœŽ=5
alpha = 0.05
mu_values = 0:6
simulation = 5000 # Generate 5000 datasets from the model

# For each dataset, save ðœ‡Ì‚ and the p-value arising from a test of ð»:ðœ‡=0 using ð›¼=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.
perform_ttest = function(mu, n, sigma, simulation, alpha) {
  results = replicate(simulation, {
  x = rnorm(n, mean = mu, sd = sigma) # Generate dataset
  test_result = t.test(x, mu = 0) # t-test
  tidy_result = broom::tidy(test_result) # cleaning output of t.test
  p_value = tidy_result$p.value
  estimate = tidy_result$estimate # estimate of the mean
  return(c(p_value = p_value, estimate = estimate))
})

results_df = data.frame(t(results)) # results to df
colnames(results_df) = c("p_value", "estimate")
  
rejection_rate = mean(results_df$p_value < alpha) # power
  
avg_estimate = mean(results_df$estimate) # average estimate
  
return(list(rejection_rate = rejection_rate, avg_estimate = avg_estimate, results = results_df))
}

power_results = lapply(mu_values, function(mu) { # simulation
  perform_ttest(mu, n, sigma, simulation, alpha)
})

rejection_rates = sapply(power_results, function(res) res$rejection_rate) # extracting rejection rates
avg_estimates = sapply(power_results, function(res) res$avg_estimate) # extracting average estimates

```

```{r q2_b, echo = FALSE, message = FALSE, warning = FALSE}
power_df = data.frame(mu = mu_values, power = rejection_rates)

power_plot = ggplot(power_df, aes(x = mu, y = power)) +
  geom_line(color = "#ade8f8")
  geom_point()
  labs(title = "Power of the Test vs. True Mean (mu)",
       x = "True Mean (mu)",
       y = "Power (Proportion Rejected)")
  
power_plot
```

The power increases for effect sizes 0 to 4, however, for effect sizes 4 to 6, the power stayed relatively similar. The increase of power was greatest for effect size 1 to 3. With the increase in effect size, there is an increase of power, which in turn indicates higher probability of rejecting the null hypothesis.

```{r q2_c, echo = FALSE, message = FALSE, warning = FALSE}
plot_data = data.frame(
  mu = mu_values,
  avg_estimate = avg_estimates,
  rejection_rates = rejection_rates
)

plot = ggplot(plot_data, aes(x = mu)) +
      # Plot the average estimate
  geom_line(aes(y = avg_estimate, color = "Average Estimate")) +
  geom_point(aes(y = avg_estimate, color = "Average Estimate")) +
      # Plot the average estimate when the null was rejected
  
  geom_line(aes(y = rejection_rates, color = "Rejection Rate"), linetype = "dashed") +
  geom_point(aes(y = rejection_rates, color = "Rejection Rate")) +
  labs(title = "Average Estimate of ÂµÌ‚ vs. True Âµ with Rejection Condition",
       x = "True Mean (Âµ)",
       y = "Estimated Mean (ÂµÌ‚)",
       color = "Legend")

plot
```

The sample average ðœ‡ across tests for which the null is rejected is not approximately equal to the true value of ðœ‡. This may be due to sample variability and Type I error.
```{r q3_a, echo = FALSE, message = FALSE, warning = FALSE}
  # importing data
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"
data_original = read.csv(url) %>% 
  janitor::clean_names()

data_original

# Assuming the dataset has columns 'city', 'state', 'disposition', and 'homicide_id'
# If the dataset is already in your environment, move to step 2.

# 2. Create a new city_state variable (city + state)
homicide_data <- homicide_data %>%
  mutate(city_state = paste(city, state, sep = ", "))

# 3. Summarize within cities to get total homicides and unsolved homicides
city_summary <- homicide_data %>%
  group_by(city_state) %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

# 4. Estimate the proportion of unsolved homicides for Baltimore, MD
baltimore_data <- city_summary %>% filter(city_state == "Baltimore, MD")
prop_test_baltimore <- prop.test(baltimore_data$unsolved_homicides, baltimore_data$total_homicides)

# Tidy the prop.test output and extract the proportion and confidence intervals
baltimore_prop <- tidy(prop_test_baltimore) %>%
  select(estimate, conf.low, conf.high)

# 5. Run prop.test for each city and extract proportions and confidence intervals
city_props <- city_summary %>%
  pmap_dfr(function(city, total, unsolved) {
    prop_test <- prop.test(unsolved, total)
    tidy_result <- tidy(prop_test)
    
    data.frame(
      city_state = city,
      estimate = tidy_result$estimate,
      conf.low = tidy_result$conf.low,
      conf.high = tidy_result$conf.high
    )
  })

# 6. Create a plot showing estimates and CIs for each city
ggplot(city_props, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  coord_flip() +  # Flip coordinates to make the plot more readable
  theme_minimal()
```